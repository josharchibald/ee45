{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8WsTLHynzlda"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset, Subset\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qITVdqgXtMNE"
      },
      "source": [
        "Discussion on data splits:\n",
        "1. Important for train and val splits to be separate from test split (we want the test data to be completely unseen to the model before evaluation) --> avoids overfitting, CV can be used to train hyperparameters\n",
        "2. Created batches for training and testing of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0lanhX1RWwd"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  \"\"\"\n",
        "  Build the best MNIST classifier.\n",
        "  \"\"\"\n",
        "  def __init__(self, dropout=0.4):\n",
        "    super(Net, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "      nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(16),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.fc = nn.Linear((7**2)*32, 10)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = out.reshape(out.size(0), -1)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "  def train(self, device, train_loader, optimizer, epoch, log_interval):\n",
        "    \"\"\"\n",
        "    This is your training function. When you call this function, the model is trained for 1 epoch.\n",
        "    \"\"\"\n",
        "    self.to(device)\n",
        "    loss_history = []\n",
        "    for i, data in enumerate(train_loader):\n",
        "      if i%log_interval == 0:\n",
        "        print('.', end=\"\")\n",
        "      images, labels = data\n",
        "      images = images.float().to(device)\n",
        "      labels = labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = self(images)\n",
        "      loss = self.criterion(output, labels)\n",
        "      loss_history.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    return loss_history\n",
        "\n",
        "  def test(self, device, test_loader):\n",
        "    test_total = 0\n",
        "    test_correct = 0\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    self.to(device)\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader):\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # forward pass\n",
        "            output = self(images)\n",
        "            # find accuracy\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            # find loss\n",
        "            loss = self.criterion(output, labels)\n",
        "            val_loss += loss.item()\n",
        "        val_loss /= len(test_loader)\n",
        "        val_acc = test_correct / test_total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "  def predict(self, x):\n",
        "    with torch.no_grad():\n",
        "      output = model(x.unsqueeze(0).unsqueeze(0).float())\n",
        "      prediction = torch.max(output.data, 1)\n",
        "      return int(prediction[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "18.2%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "91.4%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomAffine(0),\n",
        "    transforms.ElasticTransform(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "mnist = datasets.MNIST('data/', train=True, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1uZKIfVPoc-",
        "outputId": "d380b156-7886-446e-ea2a-44bf96734cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 99339606.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 29538086.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25525263.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5104643.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "\n",
            "Epoch 1: \n",
            "\tfold 1:.....\n",
            "\tfold 2:.....\n",
            "\tfold 3:.....\n",
            "\tfold 4:.....\n",
            "\tfold 5:.....\n",
            "Loss: 0.12557810144573253, Accuracy: 0.9619166666666666\n",
            "\n",
            "\n",
            "Epoch 2: \n",
            "\tfold 1:.....\n",
            "\tfold 2:.....\n",
            "\tfold 3:.....\n",
            "\tfold 4:.....\n",
            "\tfold 5:.....\n",
            "Loss: 0.09568356866118537, Accuracy: 0.9716833333333333\n",
            "\n",
            "\n",
            "Epoch 3: \n",
            "\tfold 1:.....\n",
            "\tfold 2:.."
          ]
        }
      ],
      "source": [
        "from os import access\n",
        "# Feel free to change these pre-sets and experiment with different values.\n",
        "# Set random seed.\n",
        "seed = 33\n",
        "# Set batch size.\n",
        "batch_size = 64\n",
        "# Set learning rate\n",
        "lr = 1.0\n",
        "# Set total number of epochs\n",
        "epochs = 10\n",
        "# Set other hyperparameters of your choice.\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pytorch has default MNIST dataloader which loads data at each iteration\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomAffine(0),\n",
        "    transforms.ElasticTransform(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST('data/', train=True, download=True, transform=data_transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Splitting the dataset for k-fold CV.\n",
        "n_folds = 5\n",
        "folds = kf = KFold(n_splits=n_folds)\n",
        "\n",
        "# Setting up the model with an optimizer.\n",
        "model = Net()\n",
        "optimizer = optim.RMSprop(model.parameters())\n",
        "log_interval = 150\n",
        "\n",
        "# Training loop\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f'\\nEpoch {epoch}: ', end=\"\")\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    for i, (train_idx, val_idx) in enumerate(folds.split(train_data)):\n",
        "      print(f'\\n\\tfold {i+1}:', end=\"\")\n",
        "      train = Subset(train_data, train_idx)\n",
        "      train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "      val = Subset(train_data, val_idx)\n",
        "      val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "      loss_history = model.train(device, train_loader, optimizer, epoch, log_interval)\n",
        "      loss, acc = model.test(device, val_loader)\n",
        "      train_loss.append(loss)\n",
        "      train_acc.append(acc)\n",
        "    avg_loss = np.sum(train_loss)/len(train_loss)\n",
        "    avg_acc = np.sum(train_acc)/len(train_acc)\n",
        "    print(f'\\nLoss: {avg_loss}, Accuracy: {avg_acc}\\n')\n",
        "    train_loss_history.append(avg_loss)\n",
        "    train_acc_history.append(avg_acc)\n",
        "\n",
        "    # You may optionally save your model at each epoch here\n",
        "    torch.save(model.state_dict, f'MNIST_Epoch {epoch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lt1nSWDOJYb"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(epochs), train_loss_history)\n",
        "plt.title('Training Loss over Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.savefig('loss_by_epoch')\n",
        "plt.figure()\n",
        "plt.plot(range(epochs), train_acc_history)\n",
        "plt.title('Training Accuracy over Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (Number Correct/1)')\n",
        "plt.savefig('accuracy_by_epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11as13F74qN7"
      },
      "outputs": [],
      "source": [
        "test_dataset = datasets.MNIST('data/', train=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "loss, acc = model.test(device, test_loader)\n",
        "print(f'Model loss: {loss}\\nModel accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nePGrtWduK-a"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "outputs = []\n",
        "for image in test_dataset.data:\n",
        "  with torch.no_grad():\n",
        "      image = image.to(device)\n",
        "      output = model(image.unsqueeze(0).unsqueeze(0).float())\n",
        "      prediction = torch.max(output.data, 1)\n",
        "      y = int(prediction[1].clone().detach())\n",
        "      predictions.append(y)\n",
        "      outputs.append(output.data.squeeze(0).cpu().clone().detach().numpy())\n",
        "predictions = np.array(predictions)\n",
        "outputs = np.array(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYo_owGmF0sn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def plot_precision_recall(recalls, precisions, label, c):\n",
        "    plt.plot(recalls, precisions, linewidth=2, color = c, label=label)\n",
        "    plt.xlabel(\"Recall\", fontsize=14)\n",
        "    plt.ylabel(\"Precision\", fontsize=14)\n",
        "    plt.axis([0,1,0,1])\n",
        "\n",
        "colors = ['#0000ff', '#3399ff', '#66ccff', '#99ccff', '#ccccff', '#808080', '#ffcccc', '#ff9999', '#ff6666', '#ff0000']\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.title(\"Precision-Recall All Classes\", fontsize=17)\n",
        "for num in range(0, 10):\n",
        "  y_true = np.array([True if x==num else False for x in test_dataset.targets])\n",
        "  y_scores = outputs[:, num]\n",
        "  precisions, recalls, thresholds = precision_recall_curve(y_true.ravel(), y_scores.ravel())\n",
        "  plot_precision_recall(recalls, precisions, f\"Class {num}\", colors[num])\n",
        "plt.legend(fontsize=12)\n",
        "plt.savefig('Precision_Recall')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qry-jEDbZBq"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "misclassified = dict()\n",
        "for i in range(len(test_dataset.targets)):\n",
        "  if(test_dataset.targets[i] != predictions[i]):\n",
        "    if test_dataset.targets[i].item() not in misclassified.keys():\n",
        "      misclassified[test_dataset.targets[i].item()] = []\n",
        "    misclassified[test_dataset.targets[i].item()].append((test_dataset.data[i], predictions[i]))\n",
        "\n",
        "for k in range(10):\n",
        "  print(f'\\n# Misclassified points for key {k}: {len(misclassified[k])}')\n",
        "  fig, ax = plt.subplots(1, 3)\n",
        "  for i, v in enumerate(random.sample(misclassified[k], 3)):\n",
        "    ax[i].imshow(v[0], cmap = 'gray')\n",
        "    ax[i].title(f'Classified as {v[1]}')\n",
        "  plt.title(f'Target: {k}')\n",
        "  plt.savefig(f'Misclassified_{k}')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2vMcml3ttZg"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "confusion_matrix = metrics.confusion_matrix(test_dataset.targets, torch.tensor(predictions))\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
        "cm_display.plot()\n",
        "plt.savefig('Confusion_Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlmXaSOPYkrG"
      },
      "outputs": [],
      "source": [
        "def last_layer(model, x):\n",
        "  x = x.to(device)\n",
        "  model = model.to(device)\n",
        "  out = model.layer1(x)\n",
        "  out = model.layer2(out)\n",
        "  out = out.reshape(out.size(0), -1)\n",
        "  out = model.dropout(out)\n",
        "  features = out  # store the features before the final layer\n",
        "  out = model.fc(out)\n",
        "  return out, features\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "features_all = np.empty((0, 7*7*32))\n",
        "labels_all = np.empty(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs, features = last_layer(model, images)\n",
        "        features_all = np.vstack((features_all, features.cpu().numpy()))\n",
        "        labels_all = np.append(labels_all, labels.numpy())\n",
        "\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "features_tsne = tsne.fit_transform(features_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5T4KkgWZPJ6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(10):\n",
        "    plt.scatter(features_tsne[labels_all==i, 0], features_tsne[labels_all==i, 1], label=str(i), color=colors[i])\n",
        "plt.legend()\n",
        "plt.savefig('tSNE')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NLG3kOkQORtV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Expected input batch_size (196) to match target batch_size (64).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     53\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m---> 54\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     56\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     57\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
            "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
            "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (196) to match target batch_size (64)."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Define the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(16*16, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 16*16)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Load the MNIST-like dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model\n",
        "model = Net().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy of the model on the {total} test images: {(correct / total) * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'mnist_model.ckpt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
